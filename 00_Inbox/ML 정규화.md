머신러닝에서 **정규화(Regularization)**는 모델이 훈련 데이터에 너무 과하게 적합되어 실제 데이터에서 성능이 떨어지는 **과적합(Overfitting)** 문제를 해결하기 위한 기법입니다.

학습 시 손실 함수(Loss Function)에 모델의 가중치(Weight) 크기에 따른 '페널티'를 추가하여, 모델이 너무 복잡해지지 않도록 가중치를 작게 조절하는 원리입니다. 대표적인 방법인 **L1 정규화(Lasso)**와 **L2 정규화(Ridge)**의 특징을 비교해 설명해 드릴게요.

---

## 1. L1 정규화 (Lasso Regularization)

L1 정규화는 가중치의 **절댓값의 합**을 페널티로 부여합니다.

- **수식적 특징:** 손실 함수 뒤에 $\lambda \sum |w|$가 붙습니다. ($\lambda$는 페널티의 강도를 조절하는 하이퍼파라미터)
    
- **주요 효과:** 어떤 가중치($w$) 값들을 **정확히 0**으로 만듭니다.
    
- **장점:** 영향력이 적은 특성(Feature)들을 제거하는 효과가 있어 **특성 선택(Feature Selection)** 기능이 포함된 셈입니다. 모델이 간결해져 해석력이 좋아집니다.
    

---

## 2. L2 정규화 (Ridge Regularization)

L2 정규화는 가중치의 **제곱의 합**을 페널티로 부여합니다.

- **수식적 특징:** 손실 함수 뒤에 $\lambda \sum w^2$이 붙습니다.
    
- **주요 효과:** 가중치 값을 전체적으로 **작게(0에 가깝게)** 만듭니다. 하지만 L1처럼 완전히 0으로 만들지는 않습니다.
    
- **장점:** 특정 가중치가 너무 커지는 것을 방지하여 모델의 **변동성을 줄이고 안정성**을 높입니다. 일반적으로 성능 면에서 L1보다 더 널리 사용됩니다.
    

---

## 3. 한눈에 비교하기

|**구분**|**L1 정규화 (Lasso)**|**L2 정규화 (Ridge)**|
|---|---|---|
|**페널티 계산**|가중치의 절댓값 합 ($|w|
|**가중치 결과**|불필요한 가중치를 **0**으로 만듦|가중치를 **전체적으로 작게** 유지|
|**주요 목적**|변수 선택 및 모델 단순화|과적합 방지 및 모델 안정화|
|**특징**|희소한(Sparse) 모델 생성|이상치(Outlier)에 대한 민감도 감소|

---

## 요약

- 데이터에 불필요한 변수가 많고 핵심적인 변수만 골라내고 싶다면 **L1(Lasso)**이 유리합니다.
    
- 데이터의 모든 변수가 고루 중요하거나, 전반적인 예측 성능의 안정성을 높이고 싶다면 **L2(Ridge)**가 더 적합합니다.
    
- 두 가지를 섞어 쓰는 방식은 **Elastic Net**이라고 부릅니다.
    

혹시 이 두 기법이 적용된 실제 파이썬(Scikit-learn) 코드를 확인해 보고 싶으신가요?