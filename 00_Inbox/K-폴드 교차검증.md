머신러닝 모델의 성능을 평가할 때 데이터가 부족하거나 특정 데이터셋에만 과적합(Overfitting)되는 문제를 해결하기 위해 사용하는 가장 대표적인 방법이 **K-폴드 교차검증(K-Fold Cross Validation)**입니다.

---

## 1. K-폴드 교차검증이란?

전체 데이터를 **K개**의 일정한 크기인 '폴드(Fold)'로 나누어, 각 폴드를 한 번씩 테스트 데이터로 사용하고 나머지를 학습 데이터로 사용하는 과정을 K번 반복하는 방법입니다.

### 작동 프로세스

1. 전체 데이터를 서로 겹치지 않는 **K개**의 그룹으로 나눕니다.
    
2. 첫 번째 그룹을 **검증 데이터(Validation Set)**로, 나머지 $K-1$개 그룹을 **학습 데이터(Training Set)**로 설정하여 모델을 학습시킵니다.
    
3. 학습된 모델을 검증 데이터로 평가하여 평가지표(예: Accuracy)를 기록합니다.
    
4. 이 과정을 검증 데이터 그룹을 바꿔가며 **K번 반복**합니다.
    
5. 최종적으로 K개의 평가 점수를 **평균**내어 모델의 성능을 확정합니다.
    

---

## 2. 왜 사용하는가? (장단점)

### 장점

- **데이터 활용 극대화:** 모든 데이터를 학습과 검증에 한 번씩 사용하므로 데이터가 적을 때 유용합니다.
    
- **일반화 성능 파악:** 특정 데이터셋에만 잘 작동하는 것이 아닌, 전체적인 모델의 안정성을 확인할 수 있습니다.
    
- **평가의 신뢰도:** 한 번의 테스트 결과보다 K번의 평균값이 훨씬 객관적입니다.
    

### 단점

- **계산 비용 증가:** 모델 학습을 K번 반복해야 하므로, 데이터가 방대하거나 모델이 무거울 경우 시간이 오래 걸립니다.
    

---

## 3. 주요 변형 방법

단순히 나누는 것 외에도 데이터의 특성에 따라 아래와 같은 기법을 섞어 씁니다.

- **Stratified K-Fold:** 분류 문제에서 타겟 레이블의 비율이 불균형할 때(예: 암 환자 1%, 정상 99%), 각 폴드에도 이 비율이 유지되도록 나누는 방식입니다.
    
- **Repeated K-Fold:** K-폴드 과정을 여러 번 반복하여 평가의 분산을 더 줄입니다.
    

---

## 4. 파이썬 코드 예시 (Scikit-learn)

Python

```
from sklearn.model_selection import KFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier

# KFold 객체 생성 (5개의 폴드)
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# 모델 및 교차 검증 수행
model = RandomForestClassifier()
scores = cross_val_score(model, X, y, cv=kfold)

print(f"각 폴드의 정확도: {scores}")
print(f"평균 정확도: {scores.mean():.4f}")
```

현재 분석 중이신 데이터의 성격(분류 또는 회귀)에 맞춰 적절한 K값이나 Stratified 적용 여부를 결정하는 것이 중요합니다. 보통 **K는 5 또는 10**을 가장 많이 사용합니다.

혹시 특정 라이브러리(Scikit-learn, PyTorch 등)에서 구현하는 구체적인 방법이 궁금하신가요? 혹은 데이터 불균형 문제 때문에 고민 중이신가요?