🔥 MoE(Mixture of Experts)란?

하나의 거대한 모델을 여러 ‘전문가(Expert) 모델’로 나누어 놓고,

입력마다 필요한 전문가만 선택적으로 활성화시키는 구조의 AI 모델입니다.

즉,

👉 전체 모델이 항상 모두 계산하지 않고

👉 필요한 부분(전문가)만 계산하므로

성능은 높이고 비용·속도는 줄일 수 있는 방식입니다.

🧠 왜 MoE가 나오게 되었나?

LLM은 점점 커지고 있죠.

하지만 모든 파라미터를 매번 계산하면:

- GPU 비용 증가
- 속도 저하
- 전력 소모 증가

MoE는 이런 문제를 해결하기 위해 등장했습니다.

🧩 MoE의 핵심 구조

MoE는 크게 3가지 요소로 구성됩니다.

1) Experts (전문가 네트워크)

여러 개의 작은 모델 집합

- 예: Expert 1은 번역, Expert 2는 추론, Expert 3은 수학…
- 실제로는 역할을 사람이 정해주지 않고 모델이 학습하며 특화됩니다.

2) Gating Network (게이트 네트워크)

“이번 입력에는 어떤 전문가를 쓸까?”를 결정하는 라우팅 역할

- Softmax 등을 사용해 확률적으로 전문가를 선택
- 보통 Top-1 또는 Top-2 전문가만 활성화

3) Sparse Activation (희소 활성화)

전문가 수가 100개여도,

실제로는 1~2개만 계산에 참여합니다.

⚙️ MoE가 주는 장점

✅ 1. 비용 절감 (Sparse 계산)

전체 파라미터는 매우 크지만 실제 계산은 일부만 사용

→ “큰 모델처럼 똑똑하면서도 계산량은 작은 모델 수준”

✅ 2. 확장성 (Scalability)

전문가 수를 늘려도 비용은 크게 증가하지 않음

→ 매우 큰 모델을 만들 수 있음

✅ 3. 전문성 향상

각 Expert가 특정 패턴에 자연스럽게 특화됨

→ 다양한 작업에서 높은 성능

⚠️ MoE의 단점

❌ 1. 훈련 난이도

- Experts가 균형 있게 학습되지 않을 수 있음 (imbalance problem)
- 라우팅 불안정성

❌ 2. 추론 시 분산 처리 필요

- 여러 전문가가 다른 GPU에 저장되기도 함
- 통신 비용이 증가

❌ 3. 모델 일관성 문제

- 전문가가 달라지면 답변 일관성이 흔들릴 수 있음  
    (하지만 최근 모델들은 많이 개선됨)

📌 MoE는 어디에 쓰이나?

요즘 대형 모델들은 MoE 기반이 점점 많아지고 있습니다.

- Google Gemini
- Google Switch Transformer
- Mixtral / Llama-3.2 MoE
- DeepSeek V3 (중국)
- 여러 오픈소스 MoE LLM들

특히 DeepSeek MoE, Mixtral MoE가 효율성이 뛰어난 것으로 유명합니다.

⚡ 예를 들어 쉽게 비유하면

‘병원’이라고 생각하세요.

하나의 의사가 모든 분야를 진료하는 것이 아니라

분야별 전문가(내과, 외과, 피부과…)가 있고

환자(입력)가 오면 접수 데스크(Gate)가

해당 분야 전문가에게 배정해 주는 구조.

→ 이것이 MoE입니다.