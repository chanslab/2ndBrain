**Overfitting(과적합)**은 머신러닝 모델이 **훈련 데이터(Training Data)에 너무 과하게 맞춰져서**, 정작 새로운 데이터나 실제 데이터(Test Data)가 들어왔을 때 제대로 된 예측을 하지 못하는 상태를 말합니다.

쉽게 비유하자면, 시험을 공부할 때 문제의 원리를 이해하는 것이 아니라 **기출문제의 정답과 글자 토씨 하나까지 통째로 외워버린 상황**과 같습니다. 이 경우, 조금만 변형된 문제가 나오면 틀리게 되죠.

---

### 1. 주요 특징

- **훈련 데이터에서는 높은 성능:** 모델이 훈련 데이터의 노이즈(오차)나 아주 세세한 특징까지 학습하기 때문에 점수가 매우 좋습니다.
    
- **실제 데이터에서는 낮은 성능:** 일반적인 패턴을 파악하지 못했기 때문에, 처음 보는 데이터에는 대응하지 못합니다. (이를 '일반화 능력이 떨어진다'고 표현합니다.)
    

### 2. 왜 발생하나요?

- **데이터 부족:** 모델이 일반적인 패턴을 찾기에 학습 데이터 양이 너무 적을 때 발생합니다.
    
- **모델의 과한 복잡도:** 풀려는 문제에 비해 모델의 층(Layer)이 너무 깊거나 파라미터가 너무 많을 때 발생합니다.
    
- **학습 횟수(Epoch) 과다:** 너무 오랫동안 반복해서 학습시키면 데이터의 미세한 노이즈까지 학습하게 됩니다.
    

---

### 3. 어떻게 해결하나요?

오버피팅을 방지하고 모델의 범용성을 높이기 위해 다음과 같은 기법들을 사용합니다.

|**해결 기법**|**설명**|
|---|---|
|**데이터 증강 (Data Augmentation)**|데이터를 변형(이미지 회전, 대칭 등)하거나 추가하여 데이터 양을 늘립니다.|
|**드롭아웃 (Dropout)**|학습 과정에서 신경망의 일부 노드를 무작위로 꺼서 특정 노드에 의존하지 않게 합니다.|
|**조기 종료 (Early Stopping)**|검증 오차가 증가하기 시작하는 시점에 학습을 멈춥니다.|
|**규제 (Regularization)**|모델의 가중치가 너무 커지지 않도록 패널티를 부여합니다. (L1, L2 규제 등)|

---

### 요약

오버피팅은 모델이 **"학습 데이터에만 정답을 맞히는 좁은 시야를 갖게 된 상태"**입니다. 좋은 인공지능 모델은 훈련 데이터뿐만 아니라 새로운 데이터에서도 유연하게 대처할 수 있는 **일반화(Generalization)** 능력을 갖춰야 합니다.

혹시 특정 알고리즘(예: 의사결정나무, 딥러닝 등)에서의 오버피팅 해결 방법이 더 궁금하신가요?